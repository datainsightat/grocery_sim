{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a7a0b3-5e6f-441d-9234-c9c1d5459c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 12:56:32.339909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 12:56:32.464759: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-07 12:56:32.464816: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-07 12:56:33.145224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 12:56:33.145476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 12:56:33.145489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6804a0-1b34-4c52-8172-f57bcf066724",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a48cd57-a42c-4de7-8eae-2622797f448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d079ccc1-4d24-4d15-939c-a3ca8d225f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141c74aa-75f3-4eab-8f02-4bedaa96a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae99e7f9-4945-4f4c-8379-8692e10f7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a58f35-9123-4200-888e-075055565c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff48d380110>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2525ae1c-55e2-4217-825b-ce39710e26ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff48b1e7e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8238c0-c503-4b01-bf98-30fcbd34ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa7bce8-bfd7-47e1-921e-443a73b8a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76cffa29-7d5d-4ae6-bd5f-d08c1bd812b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e629fb7f-3516-4683-bfdc-02358a4c7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41c8203-f7d6-4857-a037-abf520e24961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2e65a3-217a-410f-aa5c-86b4612e7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<matplotlib.image.AxesImage at 0x7f9cf04c5820>"
=======
       "<matplotlib.image.AxesImage at 0x7ff48b174390>"
>>>>>>> 4011941688457200f43acd44384bf9ffce0fc40e
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
<<<<<<< HEAD
      "image/png": "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",
=======
      "image/png": "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\n",
>>>>>>> 4011941688457200f43acd44384bf9ffce0fc40e
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 16,
   "id": "188b5855-91d3-4a7a-816c-6067de67ad7c",
>>>>>>> 4011941688457200f43acd44384bf9ffce0fc40e
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
<<<<<<< HEAD
     "output_type": "stream",
     "text": [
      "2023-02-07 12:57:32.509851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-07 12:57:32.509915: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-07 12:57:32.509943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5fb36e9a6578): /proc/driver/nvidia/version does not exist\n",
      "2023-02-07 12:57:32.510297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model(env)\n\u001b[1;32m      3\u001b[0m qt \u001b[39m=\u001b[39m Qtraining(\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel_1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m qt\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mQtraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_episodes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m game_over:\n\u001b[0;32m---> 41\u001b[0m     game_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay()\n\u001b[1;32m     43\u001b[0m dt \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseconds \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mtotal_seconds()\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mQtraining.play\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_episodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m inputs, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperience\u001b[39m.\u001b[39;49mget_data(data_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_size)\n\u001b[1;32m     72\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbase)\n\u001b[1;32m     73\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     74\u001b[0m     inputs,\n\u001b[1;32m     75\u001b[0m     targets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     79\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mExperience.get_data\u001b[0;34m(self, data_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m targets[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(env_state)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m Q_sa \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(next_env_state))\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m game_over:\n\u001b[1;32m     35\u001b[0m     targets[i, action] \u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mExperience.predict\u001b[0;34m(self, env_state)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, env_state):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(env_state, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/keras/engine/training.py:2350\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2349\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2350\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2351\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2352\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/grocery_sim/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
     "output_type": "stream",
     "text": [
      "2023-02-07 13:04:51.630108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:51.642006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:51.643828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:51.646309: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 13:04:51.647216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:51.648985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:51.650606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:52.357491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:52.359385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:52.361016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 13:04:52.362554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 358 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
>>>>>>> 4011941688457200f43acd44384bf9ffce0fc40e
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_31732/1882963764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_31732/4009101149.py\u001b[0m in \u001b[0;36mqtrain\u001b[0;34m(model, maze, **opt)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Train neural network model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             h = model.fit(\n\u001b[1;32m     67\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_31732/1568891854.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, data_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# There should be no target values for actions not taken.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_31732/1568891854.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, envstate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "qt.save('ml/model_t1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 12x12 Maze"
=======
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
>>>>>>> 4011941688457200f43acd44384bf9ffce0fc40e
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e21e0d-830b-4fa2-b137-8d6ce081506c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m103"
  },
  "kernelspec": {
   "display_name": "grocery_sim",
   "language": "python",
   "name": "grocery_sim"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
